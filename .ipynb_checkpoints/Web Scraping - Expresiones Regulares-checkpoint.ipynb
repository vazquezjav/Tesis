{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnMG4iaYzs4J",
    "outputId": "6178db59-4b31-4016-d2fd-3839bcb57271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
      "\r",
      "\u001b[K     |█▍                              | 10kB 17.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 20kB 24.2MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 30kB 24.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 40kB 26.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 51kB 25.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 61kB 28.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 71kB 18.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 81kB 19.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 92kB 17.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 102kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 112kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 122kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 133kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 143kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 153kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 163kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 174kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 184kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 194kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 204kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 215kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 225kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 235kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 245kB 17.9MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LZN2E_3Yq3Zd"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs4\n",
    "import requests \n",
    "from unidecode import unidecode\n",
    "import re\n",
    "respuesta_comentarios =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jTrZNPtWz2pC"
   },
   "outputs": [],
   "source": [
    "def separar_id_pagina_comentarios(url):\n",
    "    global respuesta_comentarios\n",
    "    split = url.split(\"=\")\n",
    "    id=int(re.search(r'\\d+', split[1]).group()) #separar numeros de string\n",
    "    return id\n",
    "\n",
    "def obtener_id_publicacion(url):\n",
    "    return url.split(\"/\")[-2]\n",
    "\n",
    "def encontrar_class_comentarios(html):\n",
    "    selector = 'div > h3 ~ div'\n",
    "    found = html.select(selector)\n",
    "    sp=str(found[0]).split('\"')\n",
    "    return sp[1]\n",
    "\n",
    "def vector_respuestas_comentarios(maximo):\n",
    "    #crear un vector con el maximo de respuestas de un comentario \n",
    "    respuesta=['1 respuesta',]\n",
    "    for j in range(2,maximo+1):\n",
    "        respuesta.append(str(j)+' respuestas')\n",
    "    #[str(j)+' respuestas' for j in range(2,maximo+1)]\n",
    "    return respuesta\n",
    "\n",
    "def obtener_comentarios_respuesta_comentarios(url):\n",
    "    #mediante web scraping optener las respuestas de los comentarios \n",
    "    #recibe la url previamente obtenida que nos direcciona a la pagina de respuestas a un comentario\n",
    "    comentarios=[]\n",
    "    res =requests.get(url)\n",
    "    soup =bs4(res.text , 'html.parser')\n",
    "    filtrado=soup.findAll('div', {'class':encontrar_class_comentarios(soup)})\n",
    "    for i in range(1,len(filtrado)):\n",
    "        respuesta_comentarios.append(filtrado[i].text)\n",
    "    #return comentarios\n",
    "\n",
    "def urls_respuestas_comentarios(html):\n",
    "    #devuelve la lista de url de las respuestas de los comentarios \n",
    "    respuesta=vector_respuestas_comentarios(20)\n",
    "    url_respuestas=[]\n",
    "    tags_a=html.findAll(\"a\")\n",
    "    for i in tags_a:\n",
    "        if i.text in respuesta:\n",
    "            if not i['href'] in url_respuestas:\n",
    "                url_respuestas.append('https://mbasic.facebook.com'+i['href'])\n",
    "    return url_respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8xNuKx3I0voI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url actual =  https://mbasic.facebook.com/photo?fbid=4374302172593929 \n",
      "\n",
      "Comentarios pagina =  2\n",
      "Url actual =  https://mbasic.facebook.com/story.php?story_fbid=4374302215927258&id=369927876364732&p=10 \n",
      "\n",
      "Anterior  https://mbasic.facebook.com/story.php?story_fbid=4374302215927258&id=369927876364732&p=0&refid=52\n",
      "Comentarios pagina =  3\n",
      "Url actual =  https://mbasic.facebook.com/story.php?story_fbid=4374302215927258&id=369927876364732&p=20&refid=52 \n",
      "\n",
      "Anterior  https://mbasic.facebook.com/story.php?story_fbid=4374302215927258&id=369927876364732&p=10&refid=52\n",
      "Comentarios pagina =  4\n"
     ]
    }
   ],
   "source": [
    "#facebook='https://www.facebook.com/astronomiaentubolsillooficial/photos/a.536622079824377/1940964692723435/'\n",
    "#id=obtener_id_publicacion(facebook)\n",
    "#4561570650533328\n",
    "#3979298638823457\n",
    "id='4374302172593929'\n",
    "url='https://mbasic.facebook.com/photo?fbid='+id\n",
    "comentarios=[]\n",
    "cont =0\n",
    "cont2 = 0\n",
    "more_comments=''\n",
    "id_siguiente, id_anterior = 0,0\n",
    "lista_urls=[]\n",
    "htmls=[]\n",
    "while True:\n",
    "    if cont >1:\n",
    "        if url in lista_urls:\n",
    "            break\n",
    "            \n",
    "    res =requests.get(url)\n",
    "    soup =bs4(res.text , 'html.parser')\n",
    "    lista_urls.append(url)\n",
    "    print(\"Url actual = \",url,\"\\n\")\n",
    "    if cont >0:\n",
    "        #id_anterior=separar_id_pagina_comentarios(url)\n",
    "        soup = soup.find('div', {'id':'m_story_permalink_view'})\n",
    "    else:\n",
    "        #unicamente para obtener los comentarios de la primera pagina de la publicacion \n",
    "        comments = soup.findAll('div', {'class':'_14ye'})\n",
    "        for i in comments:\n",
    "            if i.text=='':\n",
    "                print(\"entra\")\n",
    "            comentarios.append(i.text)\n",
    "    htmls.append(soup)\n",
    "    # para encontrar el link de direccionamiento hacia los otros comentarios \n",
    "    # en base a la etiqueta 'a' \n",
    "    if cont >0:\n",
    "        try:\n",
    "            tags_a=soup.findAll(\"a\")\n",
    "            for i in tags_a:\n",
    "                text=unidecode(i.text)\n",
    "                if text ==' Ver mas comentarios...':\n",
    "                    if cont2 == 0:\n",
    "                        more_comments=i['href']\n",
    "                    cont2 += 1\n",
    "                if text ==' Ver comentarios anteriores...':\n",
    "                    anterior=i['href']\n",
    "                    ul='https://mbasic.facebook.com'+anterior\n",
    "                    print(\"Anterior \",ul)\n",
    "        except:\n",
    "            break\n",
    "    else:\n",
    "                #obtener siguiente pagina comentarios unicamente primera pagina de comentarios \n",
    "        link2 =soup.find('div', {'class':'_55wr async_elem'})\n",
    "        try:\n",
    "            more_comments = link2.find('a').get('href')\n",
    "        except:\n",
    "            print(\"No hay mas comentarios\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    cont +=1\n",
    "    cont2 =0 #reiniciamos contador para obtener el url de la siguiente pagina 'ver mas comentarios'\n",
    "    url='https://mbasic.facebook.com'+more_comments\n",
    "    #id_siguiente = separar_id_pagina_comentarios(url)\n",
    "    \n",
    "    \n",
    "\n",
    "    #print(\"Anterior: \",id_anterior,\" Siguiente: \",id_siguiente)\n",
    "    print(\"Comentarios pagina = \",cont+1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_com=0\n",
    "for j in htmls:\n",
    "    if cont_com >0:\n",
    "        try:\n",
    "            comments = j.findAll('div', {'class':encontrar_class_comentarios(j)})\n",
    "            for i in comments:\n",
    "                if not i.text in comentarios:\n",
    "                    #print(i)\n",
    "                    comentarios.append(i.text)\n",
    "        except:\n",
    "            break\n",
    "    cont_com += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_comentarios=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://mbasic.facebook.com/comment/replies/?ctoken=4374302215927258_4374414272582719&count=1&curr&pc=1&ft_ent_identifier=4374302215927258&gfid=AQCU5mqUeA6up0Y_R4Q&__tn__=R']\n",
      "[]\n",
      "[]\n",
      "['Santy Zuñiga un ejemplo de trabajador que Jehova le bendiga salud fuerzas y más que todo ganas de seguir trabajando ']\n"
     ]
    }
   ],
   "source": [
    "for html in htmls:\n",
    "    rs=urls_respuestas_comentarios(html)\n",
    "    print(rs)\n",
    "    if  rs != []:\n",
    "        for i in rs:\n",
    "            obtener_comentarios_respuesta_comentarios(i)\n",
    "\n",
    "\n",
    "#respuesta_comentarios=[]\n",
    "print(respuesta_comentarios)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(respuesta_comentarios))\n",
    "print(len(comentarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todo=comentarios+respuesta_comentarios\n",
    "len(todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vazqu/nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\anaconda3\\\\envs\\\\tesis\\\\nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\anaconda3\\\\envs\\\\tesis\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\anaconda3\\\\envs\\\\tesis\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5c8c03293cd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Hola como estan todos hoy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tesis\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tesis\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tesis\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tesis\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tesis\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vazqu/nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\anaconda3\\\\envs\\\\tesis\\\\nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\anaconda3\\\\envs\\\\tesis\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\anaconda3\\\\envs\\\\tesis\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vazqu\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "c='Hola como estan todos hoy'\n",
    "c=c.lower()\n",
    "token= word_tokenize(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expresiones regulares \n",
    "\n",
    "import re\n",
    "\n",
    "regex = r\"[\\w,?\\\\.!\\s\\\\á\\\\í]{25,}\"\n",
    "\n",
    "test_str = htmls[0].text\n",
    "matches = re.finditer(regex, test_str, re.MULTILINE)\n",
    "\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    \n",
    "    #print (\"Match |  {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()),\" \\n\")\n",
    "    \n",
    "    for groupNum in range(0, len(match.groups())):\n",
    "        groupNum = groupNum + 1\n",
    "        \n",
    "        #print (\"Group {groupNum} found at {start}-{end}: {group}\".format(groupNum = groupNum, start = match.start(groupNum), end = match.end(groupNum), group = match.group(groupNum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrome_options =  Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "browser = webdriver.Firefox(executable_path=\"C:/Users/vazqu/Downloads/geckodriver-v0.29.1-win64/geckodriver.exe\")\n",
    "browser.get('https://commentpicker.com/facebook-post-id-finder.php')\n",
    "\n",
    "username = browser.find_elements_by_css_selector(\"input[id=facebook-post-url]\")\n",
    "username[0].send_keys('https://www.facebook.com/cerebrosoficial/photos/a.456747208498227/1019099838929625/')\n",
    "\n",
    "df=browser.find_element_by_xpath(\"//*[@id='facebook-post-url']\")\n",
    "df.send_keys(Keys.TAB*2, Keys.ENTER)\n",
    "\n",
    "#username[0].send_keys(Keys.TAB,Keys.TAB)\n",
    "#username[0].send_keys(Keys.ENTER)\n",
    "\n",
    "#bo=browser.find_element_by_xpath(\"//*[@id='get-post-id-button']\");\n",
    "#bo.click()\n",
    "\n",
    "text= browser.find_element_by_id(\"js-result\")\n",
    "cod=text.text\n",
    "type(cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "WebScraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
